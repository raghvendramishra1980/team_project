{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "\n",
    "from skimage.io import imread\n",
    "from skimage.transform import resize\n",
    "import numpy as np\n",
    "\n",
    "from tensorflow.keras.utils import normalize\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gethering image paths across folders and related data information from filename\n",
    "# each element contain patiend_id, idx5, patch X coord, patch Y coord, class of patch ( 0 - non-IDK, 1 - IDK )\n",
    "\n",
    "def list_of_data_images(img_folder='IDC_regular_ps50_idx5'):\n",
    "    \n",
    "    path = os.path.join(os.pardir, os.pardir, img_folder,'*','*','*')\n",
    "    paths = glob.glob(path)\n",
    "\n",
    "    str_preproc=[]\n",
    "    for img_path in paths:\n",
    "        img_name = os.path.basename(img_path)\n",
    "        img_name = img_name.split('_')\n",
    "\n",
    "        for i in range(2,len(img_name)):\n",
    "            img_name[i] = int(''.join(filter(str.isdigit, img_name[i])))\n",
    "        img_name.append(os.path.abspath(img_path))\n",
    "        str_preproc.append(img_name)\n",
    "\n",
    "    print(f\"Number of data elements in list: {len(str_preproc)}\")\n",
    "    return str_preproc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of data elements in list: 277524\n",
      "[['10253', 'idx5', 1001, 1001, 0, 'c:\\\\Users\\\\Igor\\\\DSI\\\\workspace\\\\team_project2\\\\IDC_regular_ps50_idx5\\\\10253\\\\0\\\\10253_idx5_x1001_y1001_class0.png'], ['10253', 'idx5', 1001, 1051, 0, 'c:\\\\Users\\\\Igor\\\\DSI\\\\workspace\\\\team_project2\\\\IDC_regular_ps50_idx5\\\\10253\\\\0\\\\10253_idx5_x1001_y1051_class0.png'], ['10253', 'idx5', 1001, 1101, 0, 'c:\\\\Users\\\\Igor\\\\DSI\\\\workspace\\\\team_project2\\\\IDC_regular_ps50_idx5\\\\10253\\\\0\\\\10253_idx5_x1001_y1101_class0.png'], ['10253', 'idx5', 1001, 1151, 0, 'c:\\\\Users\\\\Igor\\\\DSI\\\\workspace\\\\team_project2\\\\IDC_regular_ps50_idx5\\\\10253\\\\0\\\\10253_idx5_x1001_y1151_class0.png'], ['10253', 'idx5', 1001, 1201, 0, 'c:\\\\Users\\\\Igor\\\\DSI\\\\workspace\\\\team_project2\\\\IDC_regular_ps50_idx5\\\\10253\\\\0\\\\10253_idx5_x1001_y1201_class0.png'], ['10253', 'idx5', 1001, 1251, 0, 'c:\\\\Users\\\\Igor\\\\DSI\\\\workspace\\\\team_project2\\\\IDC_regular_ps50_idx5\\\\10253\\\\0\\\\10253_idx5_x1001_y1251_class0.png'], ['10253', 'idx5', 1001, 1301, 0, 'c:\\\\Users\\\\Igor\\\\DSI\\\\workspace\\\\team_project2\\\\IDC_regular_ps50_idx5\\\\10253\\\\0\\\\10253_idx5_x1001_y1301_class0.png'], ['10253', 'idx5', 1001, 1351, 0, 'c:\\\\Users\\\\Igor\\\\DSI\\\\workspace\\\\team_project2\\\\IDC_regular_ps50_idx5\\\\10253\\\\0\\\\10253_idx5_x1001_y1351_class0.png'], ['10253', 'idx5', 1001, 1501, 0, 'c:\\\\Users\\\\Igor\\\\DSI\\\\workspace\\\\team_project2\\\\IDC_regular_ps50_idx5\\\\10253\\\\0\\\\10253_idx5_x1001_y1501_class0.png'], ['10253', 'idx5', 1001, 1551, 0, 'c:\\\\Users\\\\Igor\\\\DSI\\\\workspace\\\\team_project2\\\\IDC_regular_ps50_idx5\\\\10253\\\\0\\\\10253_idx5_x1001_y1551_class0.png']]\n"
     ]
    }
   ],
   "source": [
    "list_data_img = list_of_data_images()\n",
    "print(list_data_img[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " shape: (277524, 50, 50, 3)\n"
     ]
    }
   ],
   "source": [
    "# convert images to 3 dim  numpy array\n",
    "\n",
    "img_size = ( 50, 50 )\n",
    "images_dataset = []\n",
    "for img in list_data_img:    \n",
    "    image = np.asarray(imread(img[len(img)-1] ), dtype='uint8')\n",
    "    if image.shape != (50, 50, 3):\n",
    "        image = resize( image, img_size, mode='reflect', preserve_range=True ) # resize images to given size and dimmensions\n",
    "    images_dataset.append( image )\n",
    "\n",
    "images_dataset = np.asarray( images_dataset, dtype='float32' )                     # convert to NumPy array\n",
    "images_dataset = normalize( images_dataset )\n",
    "print(f\"\\n shape: {images_dataset.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optinal step. \n",
    "### Save and load data to pkl file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Save the array to a pickle file\n",
    "# images_dataset - NumPy dataset\n",
    "# parts - split to save in separated files. More parts less memory need \n",
    "def SaveNpToPkl( filename, images_dataset, parts = 1 ):\n",
    "    start_ind = 0\n",
    "    step = images_dataset.shape[0] * parts//100\n",
    "    for i in range(0, parts):\n",
    "        filename_pkl = filename + str(i) +'.pkl'\n",
    "        start_ind = step*i\n",
    "        if i != parts-1:\n",
    "            end_ind = step*(i+1) \n",
    "        else:\n",
    "            end_ind = images_dataset.shape[0] \n",
    "    \n",
    "        print( f\" Dataset with indexes {start_ind}, {end_ind} saved to file {filename_pkl}\" )\n",
    "        with open(filename_pkl, 'wb') as f:\n",
    "            pickle.dump(images_dataset[start_ind : end_ind], f)\n",
    "\n",
    "# Load the array from the pickle file\n",
    "# parts - parts to load\n",
    "def LoadPklToNp( filename, parts=1 ):\n",
    "    total_list = []\n",
    "    paths = glob.glob(filename)\n",
    "    for path in paths[:parts]:\n",
    "        print(f\"Loading file {path}\")\n",
    "        with open(path, 'rb') as f:\n",
    "            part_array = pickle.load(f)\n",
    "        total_list += list(part_array)\n",
    "        #total_list = np.concatenate(( total_array, array), axis=0 )\n",
    "    return total_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Dataset with indexes 0, 27752 saved to file ..\\data\\processed\\img_dataset0.pkl\n",
      " Dataset with indexes 27752, 55504 saved to file ..\\data\\processed\\img_dataset1.pkl\n",
      " Dataset with indexes 55504, 83256 saved to file ..\\data\\processed\\img_dataset2.pkl\n",
      " Dataset with indexes 83256, 111008 saved to file ..\\data\\processed\\img_dataset3.pkl\n",
      " Dataset with indexes 111008, 138760 saved to file ..\\data\\processed\\img_dataset4.pkl\n",
      " Dataset with indexes 138760, 166512 saved to file ..\\data\\processed\\img_dataset5.pkl\n",
      " Dataset with indexes 166512, 194264 saved to file ..\\data\\processed\\img_dataset6.pkl\n",
      " Dataset with indexes 194264, 222016 saved to file ..\\data\\processed\\img_dataset7.pkl\n",
      " Dataset with indexes 222016, 249768 saved to file ..\\data\\processed\\img_dataset8.pkl\n",
      " Dataset with indexes 249768, 277524 saved to file ..\\data\\processed\\img_dataset9.pkl\n"
     ]
    }
   ],
   "source": [
    "# call numpy to pkl save func.\n",
    "\n",
    "# Specify relative path and the filename\n",
    "parts = 10\n",
    "filename = os.path.join( os.pardir, 'data', 'processed', 'img_dataset')\n",
    "SaveNpToPkl( filename, images_dataset, parts )\n",
    "\n",
    "filename = os.path.join( os.pardir, 'data', 'processed', 'lables_data')\n",
    "#SaveNpToPkl( filename, np.asarray(list_data_img), parts )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading file ..\\data\\processed\\img_dataset0.pkl\n",
      "Loading file ..\\data\\processed\\img_dataset1.pkl\n",
      "Loading file ..\\data\\processed\\img_dataset2.pkl\n",
      "Loading file ..\\data\\processed\\img_dataset3.pkl\n",
      "Loading file ..\\data\\processed\\img_dataset4.pkl\n",
      "Loading file ..\\data\\processed\\img_dataset5.pkl\n",
      "Loading file ..\\data\\processed\\img_dataset6.pkl\n",
      "Loading file ..\\data\\processed\\img_dataset7.pkl\n",
      "Loading file ..\\data\\processed\\img_dataset8.pkl\n",
      "Loading file ..\\data\\processed\\img_dataset9.pkl\n",
      "Loading file ..\\data\\processed\\lables_data0.pkl\n",
      "Loading file ..\\data\\processed\\lables_data1.pkl\n",
      "Loading file ..\\data\\processed\\lables_data2.pkl\n",
      "Loading file ..\\data\\processed\\lables_data3.pkl\n",
      "Loading file ..\\data\\processed\\lables_data4.pkl\n",
      "Loading file ..\\data\\processed\\lables_data5.pkl\n",
      "Loading file ..\\data\\processed\\lables_data6.pkl\n",
      "Loading file ..\\data\\processed\\lables_data7.pkl\n",
      "Loading file ..\\data\\processed\\lables_data8.pkl\n",
      "Loading file ..\\data\\processed\\lables_data9.pkl\n"
     ]
    }
   ],
   "source": [
    "# call pkl loader\n",
    "parts_load = 10\n",
    "Load_path = os.path.join( os.pardir, 'data', 'processed', 'img_dataset*' )\n",
    "images_dataset = np.asarray ( LoadPklToNp( Load_path, parts_load ), dtype='float32')\n",
    "\n",
    "Load_path = os.path.join( os.pardir, 'data', 'processed', 'lables_data*' )\n",
    "labels_dataset = np.asarray ( LoadPklToNp( Load_path, parts_load ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split( images_dataset, labels_dataset, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(list, 277524, '0')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(labels_dataset), len(labels_dataset), labels_dataset[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dsi_participant",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
